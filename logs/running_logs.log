[2025-08-04 21:33:10,158|(ERROR)| File: cli | Message: No server object found in /media/kirti/Dev/GenAI/E2E_Job_Recommender/main.py. Please either:
1. Use a standard variable name (mcp, server, or app)
2. Specify the object name with file:object syntax3. If the server creates the FastMCP object within main()    or another function, refactor the FastMCP object to be a    global variable named mcp, server, or app.]
[2025-08-04 21:33:42,830|(ERROR)| File: cli | Message: No server object found in /media/kirti/Dev/GenAI/E2E_Job_Recommender/main.py. Please either:
1. Use a standard variable name (mcp, server, or app)
2. Specify the object name with file:object syntax3. If the server creates the FastMCP object within main()    or another function, refactor the FastMCP object to be a    global variable named mcp, server, or app.]
[2025-08-04 21:34:19,095|(ERROR)| File: cli | Message: No server object found in /media/kirti/Dev/GenAI/E2E_Job_Recommender/main.py. Please either:
1. Use a standard variable name (mcp, server, or app)
2. Specify the object name with file:object syntax3. If the server creates the FastMCP object within main()    or another function, refactor the FastMCP object to be a    global variable named mcp, server, or app.]
[2025-08-04 21:46:14,546|(INFO)| File: server | Message: Processing request of type ListToolsRequest]
[2025-08-04 21:46:22,411|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:46:24,831|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:46:34,675|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:46:35,780|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:46:47,368|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:46:48,476|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:49:12,324|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:49:13,555|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:52:32,124|(INFO)| File: server | Message: Processing request of type ListToolsRequest]
[2025-08-04 21:52:39,007|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:52:40,434|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:54:06,677|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:54:08,555|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 21:55:05,022|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 21:55:06,119|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 401 Unauthorized"]
[2025-08-04 22:01:33,840|(INFO)| File: server | Message: Processing request of type ListToolsRequest]
[2025-08-04 22:01:41,200|(INFO)| File: server | Message: Processing request of type CallToolRequest]
[2025-08-04 22:01:43,193|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-04 22:01:43,495|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:43,810|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-04 22:01:44,096|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:44,398|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-04 22:01:45,477|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:45,536|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:45,677|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:46,771|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:48,062|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:49,392|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:50,690|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:51,987|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:53,272|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:54,567|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:55,853|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:57,146|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:58,422|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:01:59,706|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:00,985|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:02,249|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:03,545|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:04,862|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:06,161|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:07,436|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:08,733|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:10,013|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:11,306|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:12,607|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:13,904|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:15,200|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:16,501|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:17,781|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:19,062|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:20,356|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:21,648|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:22,950|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:24,239|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:25,512|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:26,791|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:28,071|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:29,381|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:30,674|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:31,957|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:33,262|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:34,563|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:35,853|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:37,141|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:38,428|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:39,702|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:41,008|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:42,292|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:43,580|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:44,884|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:46,168|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:46,237|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:47,455|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:48,742|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:50,025|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:51,302|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:52,595|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:53,908|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:55,226|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:56,511|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:57,809|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:02:59,098|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:00,394|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:01,682|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:02,975|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:04,253|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:05,533|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:06,815|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:08,101|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:09,429|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:10,728|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:12,009|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:13,297|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:14,591|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:15,875|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:17,158|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:18,474|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:19,752|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:21,037|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/Shie7rZ4wsjOWVSu1 "HTTP/1.1 200 OK"]
[2025-08-04 22:03:29,729|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/PvW1h0FSDCxqdEHuO/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-05 19:22:35,616|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:23:53,822|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:24:52,524|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:29:28,619|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:29:28,966|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:47:36,976|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:57:10,986|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 19:58:37,471|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:00:03,123|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 20:01:55,230|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.391916 seconds]
[2025-08-05 20:02:15,648|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.935661 seconds]
[2025-08-05 20:03:22,142|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:07:26,891|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:19:36,856|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:27:10,061|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:31:32,586|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:32:59,765|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:34:06,961|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:35:00,617|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 20:35:02,435|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:10:58,524|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:18:01,724|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:18:02,661|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:19:05,105|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:20:01,543|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:20:02,026|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:20:02,498|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:21:26,642|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:21:27,101|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:21:27,104|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:28:26,249|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:31:23,659|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:35:00,590|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:40:19,360|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:42:47,031|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-05 22:43:59,325|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:44:24,485|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:45:10,190|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-05 22:45:36,702|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:31:45,397|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:34:17,349|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:34:24,894|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:35:52,612|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:36:04,526|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:36:58,973|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:37:00,071|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:37:33,107|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:37:33,410|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:37:33,782|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:37:57,189|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:37:57,713|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:37:57,741|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 12:42:40,727|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:59:06,198|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 12:59:07,672|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:00:13,067|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 13:00:15,555|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:00:58,029|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:02:10,542|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:04:06,296|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:10:14,138|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:10:37,640|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:12:18,182|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 13:14:18,056|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 13:16:59,237|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 13:17:21,833|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:28:57,202|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 20:30:05,555|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:30:15,353|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 20:31:02,392|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:33:23,048|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:33:52,108|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:36:08,179|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:36:44,475|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 20:40:04,928|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:02:02,730|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:14:35,289|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:28:14,735|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:33:16,352|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:34:02,281|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:39:04,185|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.431865 seconds]
[2025-08-06 21:39:21,343|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.904398 seconds]
[2025-08-06 21:40:23,764|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 21:49:12,058|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 21:49:40,407|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 21:52:49,099|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:01:31,658|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:22:26,192|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:23:26,298|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:24:12,517|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:25:12,186|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:25:42,928|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:26:23,114|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:26:24,263|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:27:18,643|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:34:09,550|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:34:43,839|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:34:52,037|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:36:00,789|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:36:12,715|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:37:46,427|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:37:55,675|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:38:39,802|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:39:39,219|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:40:25,967|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:41:00,670|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:41:05,351|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:41:43,361|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:43:38,101|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:45:27,532|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:50:31,237|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:50:54,629|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:51:17,873|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:53:00,719|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:53:25,613|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 22:54:26,144|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:54:51,669|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:54:57,793|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:58:34,090|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 22:59:57,291|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:00:31,008|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:00:57,776|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:02:23,474|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:02:38,919|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:03:23,042|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:04:07,745|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:04:35,370|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:05:22,802|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:05:42,359|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:06:21,503|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:07:01,925|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:07:31,204|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:08:20,274|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:12:18,615|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:12:19,155|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:12:19,287|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 23:12:53,594|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:12:53,766|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:12:53,796|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 23:12:53,958|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:13:34,563|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:13:43,962|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:14,636|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:15,291|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:15,957|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-06 23:14:15,983|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:26,957|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:27,214|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:27,253|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-06 23:14:27,399|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:16:56,166|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:17:09,958|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:17:30,812|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:18:01,932|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:18:28,339|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:19:31,562|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:19:31,807|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:19:32,483|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:19:33,091|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:25:55,485|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:29:20,152|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:30:23,594|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:39:48,850|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:45:56,787|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:46:15,811|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-07 13:46:40,645|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:47:49,066|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:47:57,653|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:48:11,268|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:48:48,090|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-07 13:48:55,037|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:49:41,498|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:52:23,832|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:52:45,243|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:54:23,554|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:55:16,056|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 13:57:08,402|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-07 13:57:48,738|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-07 13:58:41,787|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-07 13:59:40,243|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 14:10:50,344|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 14:11:09,394|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 14:11:28,351|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 14:12:48,720|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 14:13:41,024|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:06:30,537|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:06:30,678|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:06:30,790|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:13:26,811|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:18:44,403|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:21:10,888|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:24:17,476|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:25:37,929|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:26:25,271|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:26:44,866|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:27:37,933|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:28:24,322|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:29:03,293|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:29:59,301|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:32:00,210|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:32:23,817|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 15:32:52,852|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:30:23,629|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:30:24,532|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:30:24,937|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:32:36,385|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:32:36,455|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:32:36,773|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:33:28,432|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:33:28,472|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:33:28,886|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:35:40,629|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:35:40,781|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:35:40,816|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:41:46,619|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:48:40,706|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:49:26,194|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:50:37,935|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-07 18:51:56,954|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-13 20:49:42,343|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-13 20:55:14,588|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-13 20:55:15,207|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:15,556|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-13 20:55:16,127|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:16,721|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-13 20:55:17,988|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:17,999|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-13 20:55:18,117|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-13 20:55:19,321|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:20,644|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:21,968|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:23,310|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:24,630|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:26,135|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:27,497|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:28,837|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:30,168|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:31,501|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:33,043|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:34,375|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:35,731|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:37,106|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:38,433|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:39,753|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:41,063|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:42,379|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:43,731|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:45,111|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:46,437|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:47,782|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:49,126|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:50,456|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:51,782|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:53,114|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:54,419|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:55,741|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:57,066|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:58,404|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:55:59,724|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:01,079|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:02,442|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:03,806|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:05,169|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:06,519|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:08,660|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:10,280|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:11,637|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:12,976|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:14,305|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:15,637|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:16,971|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:18,395|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:18,688|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-13 20:56:19,735|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:21,085|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:22,443|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:24,899|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:26,273|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:27,621|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:28,979|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:30,339|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:31,669|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:33,033|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:34,393|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:35,722|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:37,046|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:38,393|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:39,753|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:41,078|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:42,419|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:43,822|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:45,163|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:46,505|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:47,843|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:49,180|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:50,512|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:51,843|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:53,169|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:54,490|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:55,806|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:57,114|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:56:58,439|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:57:00,193|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/NJQqf9ht0spivgjsR "HTTP/1.1 200 OK"]
[2025-08-13 20:57:02,767|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/PvV08Lal3wBpAE5Nj/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-13 21:00:02,490|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 400 Bad Request"]
[2025-08-13 21:02:51,106|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 400 Bad Request"]
[2025-08-13 21:07:33,237|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 201 Created"]
[2025-08-13 21:07:33,595|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:33,912|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-13 21:07:34,244|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:34,551|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-13 21:07:35,789|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:35,872|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-13 21:07:36,215|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-13 21:07:37,114|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:38,455|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:40,219|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:41,622|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:42,986|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:44,321|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:45,638|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:46,961|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/koh3TWB9wmkKPcXSw "HTTP/1.1 200 OK"]
[2025-08-13 21:07:49,143|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/Si4rBLStwqYHfDEHm/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-14 13:32:20,996|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 13:35:37,066|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 13:40:02,374|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 13:40:43,003|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 13:41:08,425|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 13:42:52,558|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 13:44:06,375|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 13:49:33,067|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 13:51:12,438|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 14:04:19,336|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 14:12:12,788|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 14:13:06,037|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 14:18:30,621|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 14:18:40,970|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-14 14:18:55,395|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-14 14:19:03,522|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-14 14:19:03,859|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:04,568|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-14 14:19:05,180|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:05,542|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-14 14:19:06,742|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:06,872|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-14 14:19:06,926|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-14 14:19:08,043|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:09,430|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:10,746|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:12,113|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:13,847|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:15,989|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:17,555|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:18,857|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:20,160|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:21,454|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:22,749|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:24,044|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:25,365|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:26,665|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:27,957|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:29,270|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:30,574|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:31,879|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:33,201|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:34,514|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:35,816|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:37,118|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:38,424|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:39,722|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:41,023|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:42,324|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:43,638|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:44,940|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:46,374|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:47,682|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:49,015|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:50,299|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:51,695|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:53,071|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:54,418|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:55,734|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:57,032|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:58,350|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:19:59,711|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:01,024|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:02,374|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:03,741|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:05,044|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:06,445|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:07,546|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-14 14:20:07,942|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:09,563|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:11,218|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:12,560|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:13,909|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:15,219|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:16,568|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:17,866|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:19,377|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:20,835|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:22,238|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:23,552|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:24,899|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:26,192|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:27,558|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:28,905|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:30,218|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:31,519|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:32,915|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:34,399|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:36,005|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:37,415|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:38,762|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:40,069|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:41,366|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:42,728|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:44,042|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:45,388|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:46,703|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:48,808|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:50,101|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:51,469|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:52,777|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:54,130|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:55,460|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:56,791|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/jzBooVRTocpJydceQ "HTTP/1.1 200 OK"]
[2025-08-14 14:20:57,549|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/aeuEvBPKbO2B8b4ll/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-18 14:02:05,463|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:02:08,313|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:09,165|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:09,167|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:09,168|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:09,508|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:09,510|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:09,510|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:09,871|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:09,872|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:09,873|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:10,323|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:10,324|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:10,413|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:10,769|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:10,771|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:10,772|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:11,178|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:11,179|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:11,180|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:11,560|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:11,561|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:02:11,562|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:02:11,969|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:02:11,970|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:52,311|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:04:54,597|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:55,500|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:55,502|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:55,503|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:55,858|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:55,860|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:55,861|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:56,227|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:56,228|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:56,229|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:56,708|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:56,710|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:56,730|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:57,091|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:57,093|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:57,093|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:57,483|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:57,484|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:57,484|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:57,836|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:57,838|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:04:57,838|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:04:58,247|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:04:58,248|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:12,011|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:06:14,019|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:14,927|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:14,929|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:14,930|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:15,342|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:15,344|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:15,345|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:15,758|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:15,759|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:15,760|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:16,298|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:16,301|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:16,342|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:17,006|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:17,008|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:17,009|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:17,536|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:17,538|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:17,910|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:18,307|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:18,309|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:06:18,310|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:06:18,735|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:06:18,737|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:45,662|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:07:47,450|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:48,458|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:48,460|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:48,461|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:48,841|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:48,843|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:48,843|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:49,193|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:49,195|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:49,196|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:49,700|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:49,701|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:49,724|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:50,091|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:50,093|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:50,094|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:50,481|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:50,483|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:50,483|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:50,843|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:50,845|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:07:51,116|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:07:51,458|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:07:51,459|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:44,778|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:09:46,488|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:47,193|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:47,195|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:47,196|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:47,537|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:47,538|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:47,539|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:47,956|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:47,958|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:47,958|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:48,412|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:48,413|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:48,435|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:48,818|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:48,820|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:48,820|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:49,166|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:49,167|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:49,168|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:49,506|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:49,507|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:09:49,508|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:09:49,895|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:09:49,897|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:55,133|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:10:56,940|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:57,608|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:57,610|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:57,610|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:57,964|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:57,965|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:57,966|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:58,375|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:58,376|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:58,377|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:58,851|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:58,853|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:58,873|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:59,273|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:59,275|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:59,276|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:10:59,695|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:10:59,697|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:10:59,698|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:11:00,041|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:11:00,042|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:11:00,043|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:11:00,406|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:11:00,407|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:11,063|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:12:13,181|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:14,056|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:14,059|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:14,060|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:14,470|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:14,471|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:14,472|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:14,877|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:14,879|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:14,880|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:15,447|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:15,448|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:15,475|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:15,925|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:15,927|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:15,928|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:16,355|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:16,356|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:16,405|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:16,827|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:16,828|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:12:16,829|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:12:17,243|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:12:17,244|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:18,367|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:15:20,460|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:21,180|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:21,183|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:21,183|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:21,548|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:21,549|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:21,550|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:21,892|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:21,894|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:21,895|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:22,247|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:22,249|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:22,266|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:22,712|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:22,714|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:22,714|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:23,230|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:23,231|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:23,232|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:23,581|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:23,582|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:15:23,583|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:15:23,949|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:15:23,951|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:33,455|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 14:17:35,115|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:35,894|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:35,896|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:35,896|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:36,270|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:36,272|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:36,272|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:36,657|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:36,659|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:36,659|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:37,018|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:37,019|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:37,037|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:37,403|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:37,404|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:37,405|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:38,049|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:38,050|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:38,051|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:38,386|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:38,388|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:17:38,388|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:17:38,744|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:17:38,745|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:18:03,988|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 14:18:04,368|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 14:18:04,370|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 14:18:04,370|(INFO)| File: ask_llm | Message: Running chain for keywords.]
[2025-08-18 14:18:04,740|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Based on the resume , give me the best job titles or keyword to search for ...]
[2025-08-18 14:18:05,855|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-18 14:18:05,938|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 14:18:17,878|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-18 14:18:18,209|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:18,508|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 14:18:18,793|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:19,078|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 14:18:20,231|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:20,252|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-18 14:18:20,410|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 14:18:21,553|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:22,854|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:24,170|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:25,494|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:26,812|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:28,933|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:30,487|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:31,802|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:33,112|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:34,440|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:35,752|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:37,068|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:38,375|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:39,667|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:40,995|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:42,296|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:43,598|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:44,920|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:46,220|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:47,515|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:49,241|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:50,811|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:52,126|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:53,442|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:54,775|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:56,141|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:57,453|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:18:58,768|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:00,099|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:01,454|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:02,790|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:04,121|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:05,451|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:06,760|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:08,088|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:09,409|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:10,732|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:12,057|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:13,385|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:14,702|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:16,008|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:17,318|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:18,650|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:19,942|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:20,967|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 14:19:21,251|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:22,541|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:23,850|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:25,152|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:26,456|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:27,769|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:29,073|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:30,366|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:31,737|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:33,050|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:34,377|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:35,756|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:37,084|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:38,393|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:39,711|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:40,990|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:42,298|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:43,649|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:44,955|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:46,278|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:47,609|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:48,927|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:50,347|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:51,650|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:52,967|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:55,835|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:57,432|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:19:58,750|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:20:00,075|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/c2UbqMgih6My4XYWz "HTTP/1.1 200 OK"]
[2025-08-18 14:20:05,560|(ERROR)| File: linkedin | Message:  Error fetching LinkedIn jobs: peer closed connection without sending complete message body (incomplete chunked read)]
[2025-08-18 14:20:05,561|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 14:20:05,562|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: context must include a "request" key]
[2025-08-18 15:18:38,698|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 15:18:40,322|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:41,020|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:41,022|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:41,022|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:41,443|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:41,445|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:41,445|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:41,855|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:41,856|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:41,857|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:42,253|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:42,254|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:42,280|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:43,027|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:43,031|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:43,210|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:43,629|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:43,630|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:43,630|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:43,991|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:43,993|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:18:43,993|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:18:44,340|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:18:44,341|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:19:04,117|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:19:04,704|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:19:04,705|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:19:04,706|(INFO)| File: ask_llm | Message: Running chain for keywords.]
[2025-08-18 15:19:04,816|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Based on the resume , give me the best job titles or keyword to search for ...]
[2025-08-18 15:19:07,142|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-18 15:19:07,241|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 15:19:26,175|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 15:21:44,873|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 15:21:46,869|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:47,571|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:47,573|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:47,574|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:47,924|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:47,926|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:47,927|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:48,313|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:48,315|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:48,315|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:49,008|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:49,009|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:49,034|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:49,714|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:49,715|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:49,716|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:50,371|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:50,373|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:50,374|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:50,891|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:50,894|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:21:50,896|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:21:51,359|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:21:51,361|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:22:04,531|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:22:04,896|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:22:04,897|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:22:04,898|(INFO)| File: ask_llm | Message: Running chain for keywords.]
[2025-08-18 15:22:05,001|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Based on the resume , give me the best job titles or keyword to search for ...]
[2025-08-18 15:22:06,095|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"]
[2025-08-18 15:22:06,102|(ERROR)| File: job_recommendation | Message: Error while generating keywords: Error code: 400 - {'error': {'message': "tool call validation failed: parameters for tool Keywords did not match schema: errors: [missing properties: 'keywords']", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\n{\n  "tool_call": {\n    "id": "pending",\n    "type": "function",\n    "function": {\n      "name": "Keywords"\n    },\n    "parameters": {\n      "properties": {\n        "keywords": [\n          "Artificial Intelligence",\n          "Machine Learning",\n          "Data Science",\n          "Python",\n          "PyTorch"\n        ]\n      }\n    }\n  }\n}\n</tool-use>'}}]
[2025-08-18 15:48:40,787|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 15:48:42,419|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:43,124|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:43,126|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:43,127|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:43,567|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:43,569|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:43,569|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:43,959|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:43,961|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:43,961|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:44,375|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:44,376|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:44,401|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:44,773|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:44,775|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:44,921|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:45,273|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:45,275|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:45,275|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:45,641|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:45,642|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:48:45,643|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:48:45,990|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:48:45,991|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:49:43,832|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 15:49:43,833|(ERROR)| File: job_recommendation | Message: Error while generating keywords: 'list' object has no attribute 'keywords']
[2025-08-18 15:50:58,212|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 15:50:59,862|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:00,548|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:00,550|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:00,551|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:00,905|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:00,906|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:00,907|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:01,293|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:01,294|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:01,295|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:01,656|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:01,657|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:01,676|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:02,047|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:02,048|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:02,199|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:02,710|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:02,711|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:02,712|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:03,096|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:03,097|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:03,098|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:51:03,457|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:51:03,458|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:51:19,395|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 15:51:19,396|(ERROR)| File: job_recommendation | Message: Error while generating keywords: 'dict' object has no attribute 'keywords']
[2025-08-18 15:52:51,048|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 15:52:53,029|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:53,806|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:53,808|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:53,809|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:54,233|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:54,234|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:54,235|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:54,605|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:54,607|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:54,607|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:54,960|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:54,962|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:54,986|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:55,411|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:55,413|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:55,561|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:55,912|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:55,913|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:55,914|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:56,337|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:56,338|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:52:56,339|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 15:52:56,697|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 15:52:56,698|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 15:56:08,835|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 15:56:41,564|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 16:07:44,968|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:07:48,482|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:49,568|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:49,573|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:49,574|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:50,098|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:50,100|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:50,100|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:50,529|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:50,531|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:50,532|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:50,941|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:50,943|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:50,962|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:51,452|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:51,453|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:51,588|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:52,342|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:52,344|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:52,345|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:52,887|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:52,891|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:07:52,891|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:07:53,751|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:07:53,753|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:09:07,082|(INFO)| File: job_recommendation | Message: Keywords are generated successfully]
[2025-08-18 16:09:14,641|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 16:27:45,422|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:27:47,107|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:47,852|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:47,853|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:47,854|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:48,575|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:48,577|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:48,578|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:49,000|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:49,001|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:49,002|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:49,476|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:49,477|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:49,502|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:49,888|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:49,889|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:50,035|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:50,408|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:50,410|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:50,410|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:50,776|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:50,778|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:27:50,778|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:27:51,138|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:27:51,140|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:37:05,301|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:38:00,333|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:40:37,959|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:40:41,817|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:42,869|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:42,874|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:42,875|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:43,311|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:43,313|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:43,313|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:43,771|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:43,773|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:43,773|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:44,122|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:44,123|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:44,141|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:44,541|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:44,542|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:44,543|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:44,927|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:44,929|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:44,929|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:45,346|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:45,348|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:40:45,348|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:40:45,848|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:40:45,850|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:15,114|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:44:18,824|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:20,142|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:20,145|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:20,146|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:20,780|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:20,782|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:20,783|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:21,284|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:21,285|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:21,286|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:21,821|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:21,825|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:21,858|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:22,522|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:22,524|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:22,525|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:22,960|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:22,962|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:22,963|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:24,288|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:24,291|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:24,292|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:25,031|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:25,034|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:42,936|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:44:46,768|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:48,122|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:48,125|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:48,126|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:48,726|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:48,727|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:48,728|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:49,652|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:49,654|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:49,655|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:50,282|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:50,284|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:50,317|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:50,898|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:50,900|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:50,900|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:51,471|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:51,473|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:51,474|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:52,566|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:52,569|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:44:52,570|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:44:53,160|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:44:53,162|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:45,163|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:45:49,939|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:51,987|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:52,003|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:52,005|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:52,862|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:52,864|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:52,864|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:53,598|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:53,600|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:53,601|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:54,330|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:54,334|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:54,384|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:55,213|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:55,225|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:55,226|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:56,084|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:56,087|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:56,089|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:56,946|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:56,950|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:45:56,951|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:45:57,603|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:45:57,606|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:04,405|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:46:06,441|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:07,298|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:07,301|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:07,301|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:07,695|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:07,696|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:07,697|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:08,049|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:08,051|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:08,052|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:08,411|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:08,412|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:08,438|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:08,834|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:08,835|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:08,835|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:09,200|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:09,201|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:09,202|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:09,544|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:09,545|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:46:09,546|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:46:09,893|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:46:09,894|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:28,464|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:54:33,203|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:35,602|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:35,606|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:35,607|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:36,284|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:36,286|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:36,287|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:37,030|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:37,034|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:37,037|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:38,415|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:38,420|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:38,464|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:39,262|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:39,264|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:39,265|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:39,999|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:40,001|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:40,001|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:40,706|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:40,708|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:40,709|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:41,399|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:41,402|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:53,114|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:54:56,950|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:58,559|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:58,570|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:58,574|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:54:59,276|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:54:59,280|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:54:59,281|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:00,055|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:00,057|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:00,058|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:00,733|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:00,735|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:00,772|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:01,444|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:01,447|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:01,447|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:02,105|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:02,111|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:02,112|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:03,100|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:03,102|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:03,103|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:03,760|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:03,762|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:12,412|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:55:16,608|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:18,839|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:18,842|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:18,843|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:19,855|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:19,857|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:19,858|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:20,503|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:20,504|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:20,505|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:21,145|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:21,147|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:21,191|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:21,875|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:21,883|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:21,890|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:23,483|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:23,485|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:23,486|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:24,139|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:24,142|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:55:24,143|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:55:24,858|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:55:24,863|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:23,884|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:57:29,760|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:32,718|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:32,721|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:32,723|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:33,422|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:33,424|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:33,425|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:34,274|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:34,276|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:34,277|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:35,065|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:35,069|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:44,505|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:57:48,504|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:50,784|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:50,790|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:50,790|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:51,475|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:51,478|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:51,479|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:52,115|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:52,117|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:52,119|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:52,699|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:52,700|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:52,729|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:53,353|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:53,354|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:53,355|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:53,967|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:53,973|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:53,976|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:54,744|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:54,748|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:57:54,749|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:57:55,430|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:57:55,431|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:02,454|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:58:04,834|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:06,773|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:06,786|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:06,787|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:07,419|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:07,421|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:07,422|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:07,899|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:07,901|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:07,901|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:08,435|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:08,437|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:08,469|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:09,134|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:09,138|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:09,141|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:09,795|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:09,796|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:09,797|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:10,389|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:10,391|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:58:10,392|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:58:10,957|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:58:10,958|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:21,956|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 16:59:25,312|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:27,910|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:27,915|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:27,917|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:28,638|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:28,641|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:28,642|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:29,337|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:29,340|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:29,342|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:30,031|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:30,033|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:30,072|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:30,727|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:30,730|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:30,732|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:31,736|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:31,738|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:31,739|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:32,427|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:32,429|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:32,430|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 16:59:33,298|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 16:59:33,306|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 16:59:37,610|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 17:00:57,520|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:01:03,421|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:05,694|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:05,711|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:05,712|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:06,951|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:06,954|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:06,956|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:08,976|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:08,978|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:08,987|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:10,294|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:10,296|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:10,337|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:11,012|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:11,014|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:11,015|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:11,732|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:11,734|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:11,736|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:13,469|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:13,472|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:13,474|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:14,250|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:14,252|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:28,717|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:01:31,073|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:31,866|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:31,868|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:31,869|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:32,220|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:32,221|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:32,222|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:32,581|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:32,582|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:32,583|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:32,924|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:32,926|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:32,947|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:33,348|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:33,349|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:33,350|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:33,687|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:33,689|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:33,689|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:34,027|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:34,028|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:01:34,029|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:01:34,381|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:01:34,384|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:31,360|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:02:35,111|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:36,586|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:36,590|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:36,592|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:37,270|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:37,272|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:37,273|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:37,954|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:37,959|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:37,964|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:38,889|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:38,892|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:38,929|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:39,629|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:39,631|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:39,632|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:40,303|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:40,304|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:40,305|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:41,010|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:41,012|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:41,013|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:41,743|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:41,746|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:50,469|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:02:54,186|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:56,255|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:56,259|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:56,260|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:56,900|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:56,902|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:56,903|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:57,560|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:57,563|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:57,564|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:58,200|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:58,204|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:58,247|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:58,874|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:58,877|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:58,880|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:02:59,565|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:02:59,567|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:02:59,568|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:00,228|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:00,231|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:00,233|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:01,090|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:01,093|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:22,587|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:03:27,485|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:29,142|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:29,145|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:29,146|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:30,107|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:30,109|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:30,110|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:31,609|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:31,611|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:31,612|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:32,316|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:32,317|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:32,353|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:32,996|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:32,999|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:33,000|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:34,559|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:34,579|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:34,583|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:35,746|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:35,749|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:35,755|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:36,536|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:36,538|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:45,335|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:03:48,085|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:49,002|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:49,006|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:49,007|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:49,400|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:49,402|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:49,403|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:49,849|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:49,851|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:49,852|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:50,369|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:50,371|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:50,402|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:50,908|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:50,910|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:50,910|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:51,394|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:51,396|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:51,396|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:52,070|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:52,071|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:03:52,072|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:03:52,572|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:03:52,574|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:04:24,428|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-18 17:04:24,790|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:26,338|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 17:04:26,651|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:26,951|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 17:04:28,252|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-18 17:04:28,267|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 17:04:29,500|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:30,814|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:32,143|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:33,497|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:34,803|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:36,108|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:38,563|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:39,863|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:41,168|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:42,462|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:44,275|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:45,564|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:46,866|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:48,164|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:49,463|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:50,767|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:52,073|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:53,402|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:55,572|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:57,138|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:58,685|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:04:59,985|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:01,305|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:02,620|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:03,969|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:05,267|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:06,602|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:07,907|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:09,220|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:10,560|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:11,874|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:13,194|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:14,504|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:15,814|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:17,140|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:18,456|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:19,766|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:21,083|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:22,391|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:23,757|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:25,088|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:26,405|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:27,725|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:28,842|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 17:05:29,091|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:30,446|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:31,772|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:33,088|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:34,415|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:35,733|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:37,055|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:38,391|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:39,733|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:41,027|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:42,322|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:43,657|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:44,963|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:46,685|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:48,273|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:49,571|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:50,868|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:52,188|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:53,489|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:54,822|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:56,160|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:57,469|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:05:58,837|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:00,174|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:01,500|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:02,827|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:04,912|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:06,483|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:08,051|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:09,363|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:10,699|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:12,000|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:13,305|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:14,610|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:15,929|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:17,261|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:18,562|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:19,868|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:21,176|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:22,482|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:23,798|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:25,125|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:26,430|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:27,794|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:29,123|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:30,460|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:31,773|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:33,085|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/cGb5G3sv5vm4WChWf "HTTP/1.1 200 OK"]
[2025-08-18 17:06:34,166|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/fbJLmzEHD6RARWkFC/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-18 17:06:48,018|(INFO)| File: linkedin | Message:  Retrieved 100 items from dataset: fbJLmzEHD6RARWkFC]
[2025-08-18 17:06:48,019|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 17:06:48,019|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: list indices must be integers or slices, not str]
[2025-08-18 17:07:54,770|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:07:58,109|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:07:59,529|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:07:59,534|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:07:59,535|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:00,180|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:00,182|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:00,182|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:00,874|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:00,880|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:00,886|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:01,599|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:01,601|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:01,639|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:02,295|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:02,297|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:02,298|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:02,946|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:02,948|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:02,949|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:05,016|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:05,017|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:05,018|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:05,664|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:05,666|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:15,751|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:08:20,006|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:23,019|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:23,036|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:23,058|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:25,390|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:25,395|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:25,396|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:26,411|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:26,416|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:26,418|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:27,304|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:27,306|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:27,342|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:28,008|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:28,011|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:28,012|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:28,677|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:28,679|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:28,679|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:29,794|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:29,797|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:29,798|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:30,549|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:30,550|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:40,507|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:08:43,110|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:44,384|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:44,388|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:44,389|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:44,763|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:44,766|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:44,766|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:45,121|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:45,122|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:45,123|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:45,572|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:45,573|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:45,602|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:46,063|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:46,065|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:46,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:46,489|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:46,490|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:46,491|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:46,954|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:46,955|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:46,956|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:47,360|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:47,361|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:53,401|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:08:55,640|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:57,388|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:57,391|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:57,392|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:58,022|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:58,024|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:58,025|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:08:58,945|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:08:58,957|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:08:58,963|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:00,044|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:00,046|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:00,082|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:00,765|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:00,767|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:00,769|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:01,425|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:01,429|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:01,430|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:02,097|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:02,102|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:02,103|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:03,117|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:03,118|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:11,983|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:09:15,483|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:17,304|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:17,307|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:17,308|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:18,769|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:18,786|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:18,787|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:19,696|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:19,697|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:19,698|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:21,297|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:21,318|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:21,583|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:22,339|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:22,341|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:22,342|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:22,978|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:22,980|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:22,980|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:23,602|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:23,603|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:09:23,604|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:09:24,244|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:09:24,247|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:07,701|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:10:11,543|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:13,232|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:13,236|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:13,237|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:13,994|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:13,998|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:13,999|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:14,934|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:14,941|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:14,947|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:15,874|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:15,876|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:15,910|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:16,550|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:16,553|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:16,555|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:17,185|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:17,187|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:17,189|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:17,815|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:17,819|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:17,821|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:18,448|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:18,450|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:33,838|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 17:10:36,479|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:37,967|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:37,970|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:37,972|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:38,408|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:38,410|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:38,410|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:38,754|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:38,755|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:38,756|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:39,132|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:39,133|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:39,156|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:39,569|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:39,570|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:39,571|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:39,942|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:39,943|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:39,944|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:40,322|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:40,323|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:10:40,324|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 17:10:40,683|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 17:10:40,684|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 17:11:15,119|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN/runs "HTTP/1.1 201 Created"]
[2025-08-18 17:11:15,682|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:16,944|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 17:11:17,316|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:17,629|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/hKByXkMQaC5Qt9UMN "HTTP/1.1 200 OK"]
[2025-08-18 17:11:18,836|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:18,856|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-18 17:11:18,970|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 17:11:20,147|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:21,464|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:22,798|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:25,181|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:26,595|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:27,950|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:29,587|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:31,215|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:32,510|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:33,813|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:35,199|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:36,501|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:37,795|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:39,092|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:40,855|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:43,321|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:44,614|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:46,004|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:47,298|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:48,642|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:49,948|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:51,247|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:52,943|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:54,445|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:56,560|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:11:58,820|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:00,934|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:04,511|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:05,801|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:07,823|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:09,140|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:11,147|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:12,439|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:13,741|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:15,140|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:16,425|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:17,718|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:19,015|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:19,500|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-18 17:12:20,888|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:22,753|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:24,105|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:25,414|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:27,261|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:28,534|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:29,883|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:32,100|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:33,421|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:36,602|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:41,719|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:44,003|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:45,321|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:46,826|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:48,093|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:49,400|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:53,971|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:55,302|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:56,584|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:57,884|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:12:59,263|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:13:00,554|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/s8HFa9fdhebP7ykzD "HTTP/1.1 200 OK"]
[2025-08-18 17:13:01,085|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/WH96114XILKHjl3m0/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-18 17:13:08,497|(INFO)| File: linkedin | Message:  Retrieved 98 items from dataset: WH96114XILKHjl3m0]
[2025-08-18 17:13:08,502|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 18:48:24,811|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:50:09,957|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:50:15,868|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:17,537|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:17,541|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:17,542|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:18,404|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:18,406|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:18,407|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:18,809|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:18,811|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:18,811|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:19,203|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:19,204|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:19,260|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:19,605|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:19,606|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:19,607|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:19,972|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:19,973|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:19,974|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:20,629|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:20,632|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:20,633|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:50:21,331|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:50:21,340|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:50:39,213|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 18:50:39,288|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 18:51:35,128|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:51:40,542|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:43,195|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:43,198|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:43,206|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:44,382|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:44,384|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:44,384|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:45,437|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:45,446|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:45,447|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:46,362|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:46,364|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:46,421|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:47,329|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:47,334|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:47,336|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:48,113|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:48,116|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:48,117|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:48,820|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:48,830|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:51:48,834|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:51:49,860|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:51:49,862|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:09,643|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:52:17,196|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:20,147|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:20,151|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:20,154|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:21,676|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:21,678|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:21,678|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:22,710|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:22,712|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:22,712|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:24,029|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:24,050|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:24,139|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:25,124|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:25,126|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:25,128|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:26,357|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:26,359|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:26,363|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:27,477|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:27,481|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:27,482|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:28,947|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:28,957|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:45,325|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:52:53,782|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:52:58,368|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:52:58,378|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:52:58,379|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:00,157|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:00,159|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:00,159|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:01,316|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:01,327|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:01,328|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:02,750|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:02,755|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:02,852|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:04,526|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:04,540|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:04,546|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:05,988|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:05,993|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:06,001|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:07,723|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:07,753|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:07,769|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:53:09,446|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:53:09,461|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:53:38,381|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 18:53:38,456|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 18:57:36,190|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:57:39,425|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:40,187|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:40,189|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:40,189|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:40,590|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:40,592|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:40,593|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:41,008|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:41,010|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:41,011|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:41,424|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:41,426|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:41,451|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:41,887|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:41,889|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:41,889|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:42,357|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:42,360|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:42,361|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:42,930|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:42,932|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:42,933|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:43,862|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:43,865|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:51,990|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:57:54,132|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:55,037|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:55,039|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:55,040|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:55,442|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:55,443|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:55,444|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:55,858|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:55,860|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:55,860|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:56,316|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:56,317|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:56,342|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:56,745|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:56,747|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:56,748|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:57,155|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:57,157|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:57,157|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:57,544|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:57,546|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:57:57,547|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:57:57,957|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:57:57,959|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:58:16,233|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 18:58:16,314|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 18:59:14,520|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:59:19,617|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:21,366|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:21,370|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:21,372|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:22,034|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:22,036|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:22,038|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:22,609|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:22,611|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:22,611|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:23,073|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:23,075|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:23,101|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:23,493|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:23,494|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:23,494|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:23,931|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:23,933|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:23,934|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:24,412|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:24,414|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:24,414|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:24,828|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:24,830|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:34,381|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 18:59:37,897|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:39,088|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:39,090|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:39,091|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:39,607|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:39,611|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:39,612|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:40,552|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:40,553|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:40,554|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:41,129|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:41,130|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:41,163|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:41,766|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:41,768|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:41,769|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:42,348|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:42,350|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:42,350|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:42,959|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:42,962|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:42,967|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 18:59:43,958|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 18:59:43,960|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 18:59:59,897|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 18:59:59,974|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:01:46,808|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:01:53,649|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:01:56,109|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:01:56,127|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:01:56,145|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:01:57,439|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:01:57,441|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:01:57,442|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:01:58,475|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:01:58,494|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:01:58,495|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:01:59,923|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:01:59,928|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:00,044|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:01,071|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:01,080|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:01,081|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:02,285|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:02,288|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:02,292|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:03,698|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:03,717|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:03,721|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:05,705|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:05,712|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:29,660|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:02:38,693|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:43,054|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:43,069|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:43,071|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:46,256|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:46,258|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:46,262|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:47,892|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:47,930|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:47,931|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:49,441|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:49,474|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:49,598|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:51,937|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:51,947|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:51,948|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:53,221|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:53,223|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:53,224|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:54,181|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:54,186|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:02:54,189|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:02:56,317|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:02:56,332|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:37,439|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:03:45,490|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:48,492|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:48,494|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:48,495|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:49,826|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:49,830|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:49,832|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:51,395|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:51,409|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:51,426|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:53,242|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:53,244|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:53,355|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:54,917|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:54,922|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:54,929|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:56,059|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:56,070|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:56,071|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:03:58,348|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:03:58,350|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:03:58,351|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:00,504|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:00,507|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:25,857|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:04:30,501|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:31,770|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:31,776|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:31,777|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:32,442|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:32,443|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:32,445|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:33,089|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:33,098|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:33,099|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:33,787|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:33,789|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:33,831|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:34,545|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:34,547|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:34,548|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:35,212|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:35,214|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:35,215|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:36,208|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:36,218|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:36,219|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:04:37,276|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:04:37,278|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:04:37,343|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:04:37,435|(ERROR)| File: job_recommendation | Message: Error while cleaning job: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:04:37,435|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: 500: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:05:57,146|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:05:59,385|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:00,322|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:00,326|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:00,327|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:00,757|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:00,759|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:00,760|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:01,290|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:01,291|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:01,292|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:01,786|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:01,787|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:01,816|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:02,591|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:02,593|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:02,596|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:03,325|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:03,327|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:03,328|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:04,033|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:04,036|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:04,036|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:06:04,656|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:06:04,658|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:06:32,579|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:06:32,665|(ERROR)| File: job_recommendation | Message: Error while cleaning job: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:06:32,667|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: 500: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:13:24,871|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:13:28,969|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:30,566|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:30,569|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:30,570|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:31,750|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:31,764|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:31,768|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:32,578|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:32,580|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:32,581|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:33,252|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:33,254|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:33,279|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:33,760|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:33,762|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:33,762|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:34,285|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:34,287|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:34,288|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:34,950|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:34,953|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:34,954|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:13:35,472|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:13:35,473|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:13:50,414|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:13:50,514|(ERROR)| File: job_recommendation | Message: Error while cleaning job: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:13:50,515|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: 500: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:14:30,683|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:14:34,584|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:35,804|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:35,807|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:35,808|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:36,409|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:36,411|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:36,412|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:37,166|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:37,168|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:37,169|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:37,915|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:37,917|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:37,952|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:38,640|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:38,642|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:38,643|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:39,419|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:39,421|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:39,428|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:40,064|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:40,066|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:40,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:14:40,605|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:14:40,607|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:14:51,212|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:14:51,329|(ERROR)| File: job_recommendation | Message: Error while cleaning job: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:14:51,337|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: 500: invalid decimal literal (<unknown>, line 3)]
[2025-08-18 19:16:32,962|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:16:42,597|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:46,469|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:46,486|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:46,490|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:47,902|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:47,908|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:47,912|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:50,311|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:50,317|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:50,318|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:51,581|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:51,584|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:51,672|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:52,600|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:52,608|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:52,609|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:53,492|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:53,495|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:53,496|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:54,785|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:54,800|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:16:54,808|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:16:56,529|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:16:56,534|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:19,851|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-18 19:17:26,483|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:28,047|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:28,051|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:28,052|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:28,671|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:28,673|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:28,674|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:29,285|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:29,286|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:29,287|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:29,903|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:29,906|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:29,963|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:30,586|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:30,589|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:30,591|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:31,325|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:31,327|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:31,329|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:31,963|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:31,965|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:31,966|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-18 19:17:32,656|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-18 19:17:32,658|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-18 19:17:32,716|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:17:33,029|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:17:51,058|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:17:51,371|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:20:45,487|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:20:45,903|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:23:58,554|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:23:58,738|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:25:45,667|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:25:45,901|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:27:45,654|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:27:45,863|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:39:33,421|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:39:33,677|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:43:42,182|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:43:42,411|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:44:59,611|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:44:59,929|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:46:11,319|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:46:12,067|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:46:31,912|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:46:32,148|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:46:54,498|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:46:55,381|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:47:26,266|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:47:26,568|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:47:35,557|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:47:35,786|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:48:04,129|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:48:04,479|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:48:59,214|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:48:59,503|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:49:16,633|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:49:16,921|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:50:53,214|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:50:53,467|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:51:24,268|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:51:24,785|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:55:56,024|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:55:56,489|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:57:10,193|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:57:10,365|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:57:48,119|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:57:48,380|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:57:58,858|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:57:59,053|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 19:59:46,633|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 19:59:46,825|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:00:05,292|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:00:05,462|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:00:22,728|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:00:22,940|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:03:46,483|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:03:46,835|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:10:33,408|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:10:33,993|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:13:46,805|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:13:46,996|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:14:33,790|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:14:33,992|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:15:43,705|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:15:44,088|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:17:52,529|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:17:52,738|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-18 20:21:10,750|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-18 20:21:10,983|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 14:11:47,609|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 14:11:50,298|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:51,488|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:51,490|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:51,491|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:51,834|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:51,835|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:51,836|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:52,179|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:52,180|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:52,181|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:52,532|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:52,533|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:52,592|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:52,951|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:52,953|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:52,953|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:53,303|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:53,304|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:53,305|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:53,687|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:53,689|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:11:53,689|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:11:54,130|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:11:54,131|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:12:31,306|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 14:12:31,723|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 14:18:36,344|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 14:18:40,311|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:42,089|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:42,091|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:42,092|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:42,722|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:42,725|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:42,726|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:43,284|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:43,285|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:43,286|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:43,814|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:43,818|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:43,889|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:44,567|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:44,570|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:44,571|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:45,186|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:45,187|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:45,189|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:45,814|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:45,817|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:18:45,818|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:18:46,325|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:18:46,326|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:19:54,584|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 400 Bad Request"]
[2025-08-19 14:19:54,588|(ERROR)| File: naukri | Message:  Error fetching Naukri jobs: Input is not valid: Field input.keyword must be string]
[2025-08-19 14:19:54,592|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 14:19:54,665|(ERROR)| File: job_recommendation | Message: Error while searching job from the naukri: 'ApifyApiError' object is not iterable]
[2025-08-19 14:20:44,271|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 400 Bad Request"]
[2025-08-19 14:20:44,276|(ERROR)| File: naukri | Message:  Error fetching Naukri jobs: Input is not valid: Field input.keyword must be string]
[2025-08-19 14:20:44,277|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 14:20:44,279|(ERROR)| File: job_recommendation | Message: Error while searching job from the naukri: 'ApifyApiError' object is not iterable]
[2025-08-19 14:24:09,740|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 14:24:11,978|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:12,750|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:12,752|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:12,753|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:13,128|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:13,129|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:13,130|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:13,478|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:13,480|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:13,480|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:13,834|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:13,835|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:13,866|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:14,232|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:14,233|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:14,234|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:14,579|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:14,580|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:14,580|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:14,929|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:14,931|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:14,931|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:24:15,566|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:24:15,568|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:24:30,101|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 400 Bad Request"]
[2025-08-19 14:24:30,108|(ERROR)| File: naukri | Message:  Error fetching Naukri jobs: Input is not valid: Field input.keyword must be string]
[2025-08-19 14:24:30,114|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 14:24:30,171|(ERROR)| File: job_recommendation | Message: Error while searching job from the naukri: 'ApifyApiError' object is not iterable]
[2025-08-19 14:26:38,980|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 14:26:41,479|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:42,933|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:42,937|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:42,938|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:43,674|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:43,675|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:43,676|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:44,279|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:44,281|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:44,282|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:44,884|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:44,886|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:44,927|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:45,542|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:45,545|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:45,547|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:46,290|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:46,297|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:46,300|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:46,968|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:46,970|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:26:46,970|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:26:47,529|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:26:47,533|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:12,536|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 14:27:15,908|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:16,965|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:16,967|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:16,968|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:17,507|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:17,509|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:17,510|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:18,085|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:18,087|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:18,088|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:18,657|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:18,660|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:18,696|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:19,334|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:19,336|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:19,336|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:19,908|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:19,910|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:19,910|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:20,492|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:20,493|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:27:20,494|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 14:27:20,986|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 14:27:20,988|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 14:28:17,157|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 201 Created"]
[2025-08-19 14:28:17,488|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:17,783|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-19 14:28:18,117|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:18,505|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-19 14:28:19,744|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:19,760|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-19 14:28:19,886|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-19 14:28:21,046|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:22,349|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:23,666|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:25,017|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:26,366|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:27,729|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:29,032|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/9kOJXZn1PxpqEyxXb "HTTP/1.1 200 OK"]
[2025-08-19 14:28:31,382|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/t468EfaE5efrRvLo9/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-19 14:28:31,681|(INFO)| File: naukri | Message:  Retrieved 60 items from dataset: t468EfaE5efrRvLo9]
[2025-08-19 14:28:31,695|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:07:44,811|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 16:07:47,322|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:48,149|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:48,151|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:48,151|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:48,514|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:48,516|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:48,516|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:48,902|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:48,903|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:48,904|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:49,303|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:49,305|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:49,362|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:49,835|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:49,836|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:49,837|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:50,507|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:50,509|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:50,510|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:51,063|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:51,065|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:07:51,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:07:51,511|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:07:51,513|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:08:12,162|(INFO)| File: _client | Message: HTTP Request: POST https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk/runs "HTTP/1.1 201 Created"]
[2025-08-19 16:08:13,779|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:14,138|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-19 16:08:14,460|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:14,769|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/acts/alpcnRV9YI9lYVPWk "HTTP/1.1 200 OK"]
[2025-08-19 16:08:16,094|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC?waitForFinish=999999 "HTTP/1.1 200 OK"]
[2025-08-19 16:08:16,139|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:16,154|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC/log?stream=1&raw=1 "HTTP/1.1 200 OK"]
[2025-08-19 16:08:17,528|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:18,846|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:20,167|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:21,521|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:22,849|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:24,172|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:25,485|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:26,865|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:28,180|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/actor-runs/ovUwe7BqoN0boMuNC "HTTP/1.1 200 OK"]
[2025-08-19 16:08:29,022|(INFO)| File: _client | Message: HTTP Request: GET https://api.apify.com/v2/datasets/TzyxI6jv9GAoe4MzH/items?offset=0&limit=1000 "HTTP/1.1 200 OK"]
[2025-08-19 16:08:30,791|(INFO)| File: naukri | Message:  Retrieved 70 items from dataset: TzyxI6jv9GAoe4MzH]
[2025-08-19 16:08:30,827|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:13:11,234|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 16:13:14,865|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:15,719|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:15,720|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:15,721|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:16,159|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:16,161|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:16,161|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:16,592|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:16,595|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:16,596|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:17,117|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:17,118|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:17,144|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:17,666|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:17,669|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:17,672|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:18,095|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:18,096|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:18,098|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:18,600|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:18,602|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:18,602|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:13:19,036|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:13:19,038|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:13:27,665|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:13:27,718|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:20:24,719|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 16:20:27,013|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:27,931|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:27,932|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:27,933|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:28,291|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:28,292|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:28,293|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:28,951|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:28,956|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:28,961|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:29,483|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:29,484|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:29,508|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:29,896|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:29,898|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:29,899|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:30,310|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:30,311|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:30,312|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:30,765|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:30,766|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:30,767|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 16:20:31,258|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 16:20:31,259|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 16:20:33,713|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:20:33,758|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:21:21,256|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:21:21,306|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:40:51,138|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:40:51,174|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:43:16,664|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:43:16,688|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:43:21,833|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:43:21,872|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:44:46,517|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:44:46,554|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:48:09,758|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:48:09,792|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 16:52:48,037|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 16:52:48,078|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:00:34,661|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:00:34,699|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:04:24,093|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:04:24,169|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:06:15,955|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:06:15,979|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:10:15,310|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:10:15,343|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:17:00,937|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:17:00,990|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:24:30,773|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:24:30,802|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:27:27,666|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:27:27,700|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:27:51,792|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:27:51,828|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:28:06,363|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:28:06,396|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:28:43,156|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:28:43,200|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:28:54,155|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:28:54,183|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:30:47,601|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:30:47,689|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:31:15,340|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:31:15,381|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:32:00,196|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:32:00,238|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:32:24,676|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:32:24,798|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:32:50,679|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:32:50,711|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:33:08,747|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:33:08,783|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:33:29,227|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:33:29,262|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:35:24,490|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:35:24,528|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:36:09,568|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:36:09,599|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:37:59,875|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:37:59,899|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:38:36,832|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:38:36,870|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:38:58,078|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:38:58,108|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:39:19,103|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:39:19,145|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:39:39,575|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:39:39,603|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:39:54,047|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:39:54,075|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:40:09,443|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:40:09,498|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:40:34,321|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:40:34,365|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:41:02,088|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:41:02,128|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:41:26,900|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:41:26,929|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:42:00,869|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:42:00,921|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:42:40,165|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:42:40,196|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:43:16,043|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:43:16,066|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:43:40,788|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:43:40,814|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:43:59,453|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:43:59,499|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:44:18,392|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:44:18,440|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:46:42,813|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:46:42,849|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:47:12,351|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:47:12,396|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:48:09,247|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:48:09,278|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:50:14,154|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:50:14,195|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:51:15,043|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:51:15,085|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:51:38,597|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:51:38,660|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:52:43,194|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:52:43,237|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:53:02,168|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:53:02,199|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:53:32,596|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:53:32,685|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:54:13,587|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:54:13,615|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:54:35,894|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:54:35,935|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:55:12,785|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:55:12,825|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:56:02,075|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:56:02,107|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:56:06,102|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:56:06,142|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:56:27,330|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:56:27,367|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:56:42,855|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:56:42,895|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:57:01,299|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:57:01,327|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:57:06,248|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:57:06,281|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 17:57:29,762|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 17:57:29,796|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:19:35,678|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:19:35,732|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:22:05,190|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:22:05,386|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:22:27,262|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:22:27,299|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:23:55,806|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:23:55,843|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:24:28,890|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:24:28,919|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:29:05,109|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:29:05,147|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:53:58,805|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:53:58,852|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 18:54:42,438|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:54:42,467|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 18:54:42,470|(ERROR)| File: job_recommendation | Message: Error while searching job from the linkedin: 'linkedin.html' not found in search path: 'app/templates']
[2025-08-19 18:55:34,325|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 18:55:37,616|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:38,578|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:38,580|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:38,580|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:38,975|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:38,976|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:38,977|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:39,347|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:39,349|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:39,349|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:39,928|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:39,930|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:39,989|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:40,556|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:40,558|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:40,558|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:41,077|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:41,079|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:41,079|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:41,491|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:41,492|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:41,493|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:55:41,907|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:55:41,908|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:55:42,112|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 18:55:42,152|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 18:55:59,213|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 18:56:02,400|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:03,515|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:03,517|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:03,518|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:03,988|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:03,989|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:03,989|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:04,491|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:04,493|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:04,494|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:04,928|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:04,929|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:04,951|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:05,466|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:05,467|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:05,468|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:06,433|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:06,435|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:06,436|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:07,061|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:07,063|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:07,064|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:07,998|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:08,000|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:19,731|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 18:56:23,558|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:24,710|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:24,712|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:24,713|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:25,365|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:25,368|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:25,369|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:26,069|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:26,074|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:26,077|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:27,018|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:27,020|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:27,070|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:28,051|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:28,055|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:28,058|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:28,724|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:28,726|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:28,727|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:29,422|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:29,424|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 18:56:29,426|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 18:56:30,119|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 18:56:30,121|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:01:29,164|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:01:29,227|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:05:09,676|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:05:09,711|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:05:23,405|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:05:23,433|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:07:20,512|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:07:20,549|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:07:51,488|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:07:51,522|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:15:53,659|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 19:15:58,965|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:03,315|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:03,319|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:03,320|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:03,924|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:03,928|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:03,929|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:04,561|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:04,565|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:04,567|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:05,174|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:05,177|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:05,244|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:06,375|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:06,378|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:06,379|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:07,005|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:07,009|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:07,011|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:07,781|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:07,783|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:07,784|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:08,420|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:08,422|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:17,481|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 19:16:20,093|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:21,674|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:21,676|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:21,677|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:22,127|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:22,129|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:22,129|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:22,533|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:22,534|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:22,535|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:22,905|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:22,906|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:22,928|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:23,316|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:23,317|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:23,318|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:23,689|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:23,690|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:23,691|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:24,054|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:24,055|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:24,056|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 19:16:24,430|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 19:16:24,432|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 19:16:46,713|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:16:46,848|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:19:32,604|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:19:32,633|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:22:43,129|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:22:43,166|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:22:47,341|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:22:47,376|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 19:22:55,077|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:22:55,106|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:37:05,152|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:37:05,189|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:37:45,008|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:37:45,046|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:38:20,313|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:38:20,339|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 19:38:35,023|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:38:35,055|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 19:41:05,751|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:41:05,791|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-19 19:41:12,868|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 19:41:12,900|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:42:19,768|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-19 20:42:22,594|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:23,414|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:23,416|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:23,416|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:23,774|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:23,776|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:23,776|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:24,135|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:24,136|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:24,137|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:24,505|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:24,506|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:24,562|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:24,915|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:24,917|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:24,917|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:25,270|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:25,271|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:25,272|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:25,624|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:25,626|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:25,626|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-19 20:42:26,107|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-19 20:42:26,108|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-19 20:42:43,011|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:42:43,073|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:43:56,321|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:43:56,350|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:45:36,862|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:45:36,944|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:46:12,245|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:46:12,279|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:46:23,464|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:46:23,512|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:47:58,721|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:47:58,749|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:48:29,519|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:48:29,553|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:53:26,514|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:53:26,550|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:53:48,065|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:53:48,164|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:54:50,919|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:54:50,943|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:55:20,597|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:55:20,622|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:55:50,162|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:55:50,195|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-19 20:56:01,100|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-19 20:56:01,127|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-20 14:54:16,615|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 14:54:22,047|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:24,693|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:24,696|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:24,696|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:25,336|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:25,338|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:25,338|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:26,012|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:26,015|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:26,016|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:26,596|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:26,597|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:26,653|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:27,321|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:27,325|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:27,326|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:28,026|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:28,032|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:28,042|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:28,757|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:28,759|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:54:28,760|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:54:29,257|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:54:29,259|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:05,708|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 14:55:09,668|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:11,117|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:11,124|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:11,129|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:12,257|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:12,266|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:12,272|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:13,623|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:13,626|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:13,628|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:14,263|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:14,266|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:14,327|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:15,316|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:15,324|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:15,332|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:16,498|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:16,504|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:16,505|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:17,190|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:17,191|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 14:55:17,192|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 14:55:18,187|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 14:55:18,191|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:36,517|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 15:04:38,402|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:39,132|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:39,134|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:39,135|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:39,798|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:39,800|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:39,801|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:40,194|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:40,195|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:40,196|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:40,549|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:40,550|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:40,575|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:40,945|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:40,947|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:40,948|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:41,442|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:41,444|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:41,444|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:41,841|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:41,842|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 15:04:41,843|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 15:04:42,206|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 15:04:42,207|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:15,539|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:18:18,235|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:19,018|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:19,020|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:19,021|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:19,378|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:19,380|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:19,381|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:19,743|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:19,744|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:19,745|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:20,101|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:20,102|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:20,168|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:20,623|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:20,625|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:20,625|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:21,093|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:21,094|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:21,095|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:21,457|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:21,458|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:18:21,459|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:18:21,863|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:18:21,864|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:16,056|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:29:18,941|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:19,778|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:19,780|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:19,781|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:20,171|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:20,172|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:20,173|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:20,534|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:20,535|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:20,536|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:20,897|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:20,899|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:20,947|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:21,389|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:21,391|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:21,392|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:21,804|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:21,806|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:21,806|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:22,241|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:22,242|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:29:22,243|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:29:22,616|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:29:22,617|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:27,523|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:35:31,162|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:33,567|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:33,570|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:33,572|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:34,245|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:34,247|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:34,248|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:35,040|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:35,043|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:35,045|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:36,025|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:36,027|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:36,110|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:36,965|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:36,966|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:36,967|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:37,852|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:37,854|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:37,854|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:38,515|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:38,519|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:38,521|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:39,230|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:39,232|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:52,328|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:35:58,140|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:35:59,450|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:35:59,454|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:35:59,455|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:00,114|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:00,116|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:00,117|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:00,775|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:00,778|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:00,779|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:01,457|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:01,462|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:01,520|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:02,166|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:02,175|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:02,186|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:03,230|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:03,232|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:03,233|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:03,868|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:03,869|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:03,870|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:04,475|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:04,477|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:15,234|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:36:21,481|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:23,269|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:23,276|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:23,277|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:24,159|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:24,163|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:24,166|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:25,025|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:25,027|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:25,027|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:25,729|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:25,731|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:25,776|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:27,011|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:27,022|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:27,023|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:27,789|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:27,791|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:27,792|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:28,434|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:28,436|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:28,436|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:29,850|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:29,853|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:41,966|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:36:44,822|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:45,785|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:45,787|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:45,788|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:46,349|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:46,350|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:46,351|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:46,989|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:46,993|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:46,994|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:47,426|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:47,427|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:47,455|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:48,031|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:48,034|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:48,035|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:48,635|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:48,637|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:48,638|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:49,272|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:49,273|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:36:49,274|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:36:49,726|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:36:49,728|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:24,062|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:37:26,809|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:27,708|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:27,710|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:27,710|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:28,225|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:28,227|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:28,227|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:28,628|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:28,630|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:28,631|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:29,130|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:29,132|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:29,198|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:29,920|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:29,922|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:29,923|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:30,851|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:30,855|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:30,858|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:32,700|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:32,705|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:32,706|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:33,436|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:33,438|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:51,209|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:37:55,185|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:56,404|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:56,408|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:56,409|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:57,045|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:57,048|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:57,049|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:57,649|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:57,651|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:57,652|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:58,391|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:58,395|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:58,443|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:37:59,442|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:37:59,446|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:37:59,446|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:00,046|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:00,047|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:00,048|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:00,640|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:00,642|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:00,644|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:01,128|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:01,129|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:07,637|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:38:09,722|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:10,424|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:10,426|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:10,427|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:10,770|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:10,772|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:10,772|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:11,126|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:11,127|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:11,128|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:11,465|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:11,467|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:11,489|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:11,856|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:11,857|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:11,858|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:12,215|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:12,217|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:12,217|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:12,556|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:12,557|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:38:12,558|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:38:12,913|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:38:12,914|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:28,339|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:40:31,677|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:33,012|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:33,015|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:33,016|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:33,677|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:33,680|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:33,681|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:35,024|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:35,026|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:35,027|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:35,747|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:35,749|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:35,790|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:36,464|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:36,465|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:36,466|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:37,100|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:37,102|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:37,103|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:38,464|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:38,479|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:38,489|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:39,537|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:39,539|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:47,123|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:40:49,897|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:51,222|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:51,224|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:51,225|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:51,668|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:51,670|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:51,670|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:52,054|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:52,056|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:52,057|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:52,574|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:52,576|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:52,600|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:53,186|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:53,189|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:53,191|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:53,714|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:53,717|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:53,718|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:54,160|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:54,162|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:40:54,162|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:40:54,596|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:40:54,597|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:21,351|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:42:24,379|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:25,116|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:25,118|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:25,119|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:25,537|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:25,540|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:25,541|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:25,982|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:25,983|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:25,984|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:26,431|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:26,433|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:26,473|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:27,019|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:27,022|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:27,024|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:27,413|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:27,415|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:27,417|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:27,806|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:27,807|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:42:27,808|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:42:28,159|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:42:28,160|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:11,051|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:45:15,505|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:17,506|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:17,510|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:17,512|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:18,152|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:18,153|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:18,154|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:18,865|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:18,868|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:18,870|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:19,531|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:19,533|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:19,578|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:20,294|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:20,295|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:20,296|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:20,996|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:20,998|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:20,999|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:21,640|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:21,641|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:21,642|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:22,354|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:22,356|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:34,817|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:45:38,948|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:39,905|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:39,907|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:39,908|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:40,313|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:40,314|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:40,315|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:40,851|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:40,853|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:40,855|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:42,000|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:42,005|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:42,058|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:42,518|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:42,520|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:42,520|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:43,008|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:43,010|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:43,010|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:43,563|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:43,566|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:43,568|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:44,112|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:44,114|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:53,544|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:45:56,116|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:57,298|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:57,302|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:57,304|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:57,928|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:57,932|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:57,933|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:58,574|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:58,579|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:58,582|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:59,047|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:59,048|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:59,077|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:45:59,561|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:45:59,565|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:45:59,566|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:00,249|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:00,254|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:00,255|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:01,066|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:01,068|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:01,069|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:01,659|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:01,662|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:23,663|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:46:27,325|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:28,741|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:28,751|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:28,752|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:29,424|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:29,427|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:29,428|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:30,230|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:30,234|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:30,235|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:31,374|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:31,377|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:31,423|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:32,095|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:32,098|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:32,100|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:32,944|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:32,946|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:32,947|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:33,809|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:33,811|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:33,812|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:34,853|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:34,855|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:51,216|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:46:54,635|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:55,933|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:55,935|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:55,936|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:56,581|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:56,583|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:56,585|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:57,245|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:57,247|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:57,248|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:58,911|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:58,915|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:58,966|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:46:59,652|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:46:59,654|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:46:59,654|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:00,350|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:00,353|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:00,354|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:01,025|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:01,027|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:01,028|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:01,668|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:01,670|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:10,676|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:47:14,189|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:15,478|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:15,482|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:15,483|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:16,488|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:16,490|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:16,491|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:17,030|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:17,031|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:17,032|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:17,473|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:17,475|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:17,498|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:17,922|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:17,924|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:17,924|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:18,342|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:18,343|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:18,344|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:18,741|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:18,743|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:47:18,743|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:47:19,138|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:47:19,140|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:49:57,946|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:50:01,291|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:03,091|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:03,094|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:03,095|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:03,742|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:03,744|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:03,746|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:04,356|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:04,359|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:04,360|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:04,991|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:04,993|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:05,030|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:05,725|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:05,728|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:05,730|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:06,456|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:06,458|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:06,459|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:07,636|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:07,639|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:07,640|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:08,274|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:08,276|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:19,442|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:50:21,592|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:22,353|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:22,355|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:22,356|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:22,741|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:22,742|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:22,742|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:23,102|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:23,103|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:23,104|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:23,436|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:23,437|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:23,462|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:23,840|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:23,842|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:23,842|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:24,199|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:24,201|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:24,201|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:24,580|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:24,581|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:50:24,581|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:50:24,930|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:50:24,931|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:51:24,352|(INFO)| File: ask_llm | Message: Running chain for suggestions.]
[2025-08-20 16:51:24,746|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Carefully analyze the following resume and identify **missing skills** that...]
[2025-08-20 16:51:24,762|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Analyze the following resume. Based on the candidate's skills, education, a...]
[2025-08-20 16:51:24,779|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Analyze the provided resume and identify actionable recommendations to enha...]
[2025-08-20 16:51:24,826|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.442006 seconds]
[2025-08-20 16:51:24,830|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.457564 seconds]
[2025-08-20 16:51:24,833|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.464758 seconds]
[2025-08-20 16:51:25,284|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.944614 seconds]
[2025-08-20 16:51:25,295|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.837507 seconds]
[2025-08-20 16:51:25,305|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.966886 seconds]
[2025-08-20 16:53:16,304|(INFO)| File: ask_llm | Message: Running chain for suggestions.]
[2025-08-20 16:53:16,330|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Analyze the following resume. Based on the candidate's skills, education, a...]
[2025-08-20 16:53:16,330|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Carefully analyze the following resume and identify **missing skills** that...]
[2025-08-20 16:53:16,367|(INFO)| File: llm_chain | Message: Prompt loaded and rendered: Analyze the provided resume and identify actionable recommendations to enha...]
[2025-08-20 16:53:24,543|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-20 16:53:33,306|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 502 Bad Gateway"]
[2025-08-20 16:53:33,308|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.409448 seconds]
[2025-08-20 16:53:34,127|(INFO)| File: _base_client | Message: Retrying request to /openai/v1/chat/completions in 0.478233 seconds]
[2025-08-20 16:53:37,506|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-20 16:53:37,530|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"]
[2025-08-20 16:53:37,554|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 16:59:25,198|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:59:27,722|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:28,737|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:28,739|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:28,740|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:29,258|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:29,260|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:29,261|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:29,803|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:29,805|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:29,806|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:30,390|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:30,392|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:30,431|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:31,066|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:31,070|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:31,071|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:31,658|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:31,660|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:31,660|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:32,186|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:32,189|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:32,191|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:32,885|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:32,887|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:44,005|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 16:59:48,881|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:50,222|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:50,226|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:50,227|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:50,951|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:50,954|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:50,956|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:51,771|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:51,773|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:51,774|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:53,212|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:53,230|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:53,490|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:54,655|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:54,659|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:54,660|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:55,327|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:55,330|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:55,332|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:55,979|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:55,982|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 16:59:55,984|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 16:59:56,765|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 16:59:56,769|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:08,263|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:00:12,127|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:13,403|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:13,405|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:13,407|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:14,098|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:14,100|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:14,101|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:14,814|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:14,816|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:14,817|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:15,457|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:15,459|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:15,504|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:16,161|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:16,163|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:16,163|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:16,805|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:16,808|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:16,809|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:17,452|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:17,453|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:17,454|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:18,090|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:18,092|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:29,539|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:00:32,623|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:33,373|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:33,374|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:33,375|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:33,710|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:33,712|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:33,712|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:34,062|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:34,064|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:34,064|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:34,481|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:34,482|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:34,508|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:34,944|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:34,945|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:34,946|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:35,382|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:35,383|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:35,384|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:35,762|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:35,763|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:35,764|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:36,114|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:36,116|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:43,804|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:00:49,362|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:50,531|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:50,534|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:50,536|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:51,155|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:51,156|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:51,157|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:51,568|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:51,571|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:51,572|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:52,089|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:52,090|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:52,116|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:52,521|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:52,523|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:52,523|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:52,892|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:52,894|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:52,895|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:53,372|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:53,373|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:00:53,374|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:00:53,731|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:00:53,733|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:00,025|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:01:02,219|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:03,455|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:03,466|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:03,476|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:04,505|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:04,508|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:04,510|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:05,631|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:05,633|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:05,634|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:06,282|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:06,285|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:06,330|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:06,914|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:06,916|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:06,916|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:07,541|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:07,543|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:07,545|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:08,103|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:08,104|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:08,106|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:08,542|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:08,544|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:15,903|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:01:18,289|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:19,130|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:19,132|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:19,133|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:19,534|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:19,537|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:19,538|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:19,951|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:19,953|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:19,954|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:20,388|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:20,390|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:20,416|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:20,801|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:20,803|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:20,803|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:21,214|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:21,215|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:21,216|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:21,590|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:21,591|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:01:21,592|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:01:22,029|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:01:22,031|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:39,359|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:03:41,403|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:42,131|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:42,133|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:42,134|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:42,522|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:42,524|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:42,524|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:42,893|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:42,894|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:42,895|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:43,249|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:43,250|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:43,283|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:43,684|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:43,687|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:43,688|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:44,119|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:44,121|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:44,122|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:44,499|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:44,500|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:03:44,501|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:03:44,902|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:03:44,903|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:07,567|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:04:10,093|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:10,798|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:10,800|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:10,800|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:11,155|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:11,157|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:11,158|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:11,531|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:11,532|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:11,533|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:11,913|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:11,915|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:11,953|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:12,431|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:12,433|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:12,436|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:12,893|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:12,895|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:12,896|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:13,253|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:13,254|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:13,255|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:13,656|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:13,658|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:20,388|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:04:22,686|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:23,444|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:23,446|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:23,447|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:23,790|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:23,792|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:23,792|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:24,154|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:24,156|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:24,156|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:24,585|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:24,586|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:24,607|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:24,996|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:24,998|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:24,999|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:25,396|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:25,398|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:25,398|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:25,742|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:25,743|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:04:25,744|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:04:26,099|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:04:26,100|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:30,850|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:05:33,727|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:34,904|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:34,906|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:34,908|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:35,543|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:35,547|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:35,550|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:36,222|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:36,232|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:36,238|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:36,782|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:36,783|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:36,820|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:37,249|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:37,251|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:37,252|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:38,055|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:38,057|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:38,058|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:38,560|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:38,562|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:05:38,563|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:05:38,988|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:05:38,989|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:06:47,956|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 17:08:10,041|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:08:13,141|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:14,530|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:14,533|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:14,534|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:15,145|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:15,147|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:15,148|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:15,760|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:15,762|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:15,762|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:16,363|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:16,364|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:16,420|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:17,064|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:17,065|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:17,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:17,709|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:17,711|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:17,711|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:18,309|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:18,311|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:18,311|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:08:19,099|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:08:19,101|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:08:38,296|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 17:09:01,847|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:09:04,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:05,112|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:05,116|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:05,118|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:05,685|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:05,687|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:05,687|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:06,028|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:06,030|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:06,030|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:06,387|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:06,388|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:06,417|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:06,776|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:06,777|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:06,778|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:07,233|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:07,234|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:07,235|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:08,135|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:08,138|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:09:08,140|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:09:08,575|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:09:08,576|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:16:32,532|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:16:36,619|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:16:37,914|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:16:37,917|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:16:37,918|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:16:38,556|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:16:38,558|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:16:38,560|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:16:39,199|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:16:39,201|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:16:39,202|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:16:39,829|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:16:39,831|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:16:55,512|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:17:00,715|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:03,631|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:03,657|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:03,660|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:04,364|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:04,366|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:04,366|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:05,014|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:05,016|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:05,017|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:05,652|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:05,654|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:19,997|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:17:22,036|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:22,980|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:22,983|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:22,983|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:23,714|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:23,716|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:23,716|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:24,286|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:24,288|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:24,289|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:25,027|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:25,037|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:38,015|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:17:41,436|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:42,687|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:42,690|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:42,691|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:43,326|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:43,330|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:43,331|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:43,972|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:43,973|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:17:43,974|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:17:44,631|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:17:44,634|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:02,259|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:18:05,433|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:06,724|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:06,727|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:06,728|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:07,389|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:07,392|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:07,393|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:08,652|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:08,653|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:08,654|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:09,314|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:09,316|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:09,357|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:10,018|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:10,020|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:10,021|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:10,707|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:10,708|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:10,709|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:11,377|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:11,379|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:11,380|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:12,048|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:12,051|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:27,481|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:18:30,802|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:32,054|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:32,056|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:32,057|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:33,463|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:33,465|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:33,466|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:34,425|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:34,432|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:34,433|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:36,043|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:36,048|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:46,921|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:18:49,111|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:49,970|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:49,972|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:49,973|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:50,375|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:50,376|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:50,376|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:50,763|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:50,765|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:18:50,766|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:18:51,263|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:18:51,265|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:52,003|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:20:55,644|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:56,916|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:56,919|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:56,920|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:57,278|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:57,280|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:57,280|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:57,628|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:57,630|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:57,630|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:57,996|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:57,997|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:58,050|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:58,591|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:58,592|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:58,593|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:59,059|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:59,060|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:59,060|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:59,436|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:59,438|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:20:59,438|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:20:59,932|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:20:59,934|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:22:33,985|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 17:22:33,986|(INFO)| File: helper | Message: Parsed using JSON]
[2025-08-20 17:35:16,419|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:35:20,931|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:22,062|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:22,066|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:22,067|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:22,754|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:22,758|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:22,762|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:23,437|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:23,439|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:23,440|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:24,071|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:24,074|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:24,117|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:25,258|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:25,261|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:25,262|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:25,872|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:25,875|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:25,876|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:26,508|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:26,510|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:26,512|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:27,166|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:27,168|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:37,458|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:35:41,419|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:42,855|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:42,858|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:42,859|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:43,578|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:43,580|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:43,581|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:44,405|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:44,408|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:44,409|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:45,073|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:45,075|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:45,111|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:45,806|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:45,810|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:45,811|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:46,468|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:46,470|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:46,471|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:47,022|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:47,024|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:35:47,024|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:35:47,625|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:35:47,628|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:33,048|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:36:37,205|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:38,465|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:38,468|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:38,469|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:39,111|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:39,113|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:39,113|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:39,807|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:39,810|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:39,811|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:40,449|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:40,451|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:40,496|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:41,105|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:41,107|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:41,108|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:41,734|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:41,736|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:41,736|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:42,419|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:42,421|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:42,422|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:36:43,151|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:36:43,153|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:36:55,401|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:36:59,166|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:00,616|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:00,618|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:00,619|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:01,117|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:01,118|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:01,119|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:01,676|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:01,679|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:01,683|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:02,178|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:02,180|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:02,202|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:02,587|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:02,589|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:02,590|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:03,037|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:03,040|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:03,040|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:03,623|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:03,627|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:03,630|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:37:04,070|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:37:04,072|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:37:41,532|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 17:38:10,615|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:38:14,466|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:15,926|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:15,936|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:15,944|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:17,005|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:17,008|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:17,009|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:17,642|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:17,645|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:17,646|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:18,361|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:18,370|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:18,489|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:19,614|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:19,616|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:19,618|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:20,822|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:20,825|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:20,827|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:21,513|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:21,515|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:38:21,516|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:38:22,383|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:38:22,385|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:09,834|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:39:13,163|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:14,826|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:14,830|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:14,834|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:15,536|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:15,539|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:15,541|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:16,392|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:16,399|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:16,410|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:18,026|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:18,028|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:18,066|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:18,709|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:18,712|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:18,713|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:19,952|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:19,953|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:19,954|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:20,833|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:20,841|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:20,847|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:21,745|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:21,747|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:35,354|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:39:37,830|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:38,921|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:38,923|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:38,924|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:39,760|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:39,764|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:39,769|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:40,568|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:40,573|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:40,578|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:41,363|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:41,365|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:41,409|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:42,059|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:42,061|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:42,062|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:42,697|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:42,700|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:42,701|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:43,252|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:43,254|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:43,254|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:43,838|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:43,842|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:52,160|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-20 17:39:54,609|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:55,426|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:55,428|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:55,428|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:55,842|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:55,843|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:55,845|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:56,254|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:56,255|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:56,256|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:56,689|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:56,691|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:56,715|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:57,155|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:57,157|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:57,157|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:57,550|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:57,552|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:57,552|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:57,955|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:57,957|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:39:57,957|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-20 17:39:58,468|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-20 17:39:58,470|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-20 17:49:04,205|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:01:09,990|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:04:05,266|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:06:51,713|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:10:32,170|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:14:57,520|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:24:48,115|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:32:34,444|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:34:29,007|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:37:24,353|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:38:31,340|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:41:04,911|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:41:52,733|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:45:30,433|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 18:56:57,148|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:02:06,059|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:06:52,863|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:07:59,806|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:12:51,061|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:13:38,918|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:14:46,415|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:16:11,600|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:16:18,974|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:16:51,320|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:18:06,187|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:18:34,115|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:20:29,321|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:28:10,910|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:33:31,613|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:38:36,552|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:40:37,372|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 19:40:39,777|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-20 19:40:58,748|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 19:40:59,343|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-20 19:44:41,928|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:44:58,532|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 19:46:14,859|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 19:46:17,420|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-20 19:46:41,218|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 19:46:42,989|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-20 19:46:49,759|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 19:46:50,362|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-20 20:21:39,123|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-20 20:21:50,592|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-20 20:21:51,140|(INFO)| File: job_recommendation | Message: Jobs from naukri fetched successfully!]
[2025-08-21 13:17:15,457|(INFO)| File: helper | Message: Loaded APIFY API key from environment variables.]
[2025-08-21 13:17:18,265|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:19,106|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:19,109|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:19,112|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:19,629|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:19,630|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:19,631|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:20,006|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:20,008|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:20,008|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:20,366|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:20,367|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:20,423|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:20,796|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:20,797|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:20,798|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:21,320|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:21,321|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:21,322|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:21,775|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:21,776|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:17:21,777|(INFO)| File: llm_model | Message: Model initialized with name: llama3-8b-8192, temperature: 0.5, max_tokens: 550]
[2025-08-21 13:17:22,171|(INFO)| File: llm_model | Message: Chat prompt template created.]
[2025-08-21 13:17:22,172|(INFO)| File: llm_chain | Message: Chain created successfully.]
[2025-08-21 13:18:03,278|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-21 13:18:40,863|(INFO)| File: helper | Message: Parsed using ast.literal_eval]
[2025-08-21 13:18:43,243|(INFO)| File: job_recommendation | Message: Jobs from LinkedIn fetched successfully!]
[2025-08-21 13:31:22,435|(INFO)| File: suggestions | Message: Suggestion generated successfully]
[2025-08-21 16:01:50,199|(INFO)| File: jobs_cache | Message: Job response cached for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 16:01:50,233|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 19:32:12,698|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 19:34:42,132|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 19:34:42,161|(INFO)| File: suggestions_cache | Message: No cached suggestions found for file hash: 891114b445...]
[2025-08-21 19:34:42,165|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 19:36:17,115|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 19:36:17,122|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 19:47:35,479|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 19:47:35,507|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 19:47:35,511|(INFO)| File: keywords_cache | Message: No cached keywords found for file hash: 891114b445...]
[2025-08-21 19:47:35,519|(INFO)| File: keywords_cache | Message: Keywords saved to cache for file hash: 891114b445...]
[2025-08-21 19:47:35,527|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:06:18,565|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:06:18,574|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:06:18,579|(INFO)| File: keywords_cache | Message: No cached keywords found for file hash: 891114b445...]
[2025-08-21 20:06:18,584|(INFO)| File: keywords_cache | Message: Keywords saved to cache for file hash: 891114b445...]
[2025-08-21 20:06:18,589|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:06:18,616|(INFO)| File: resume_cache | Message: No cached resume found for file hash: 7f03bf2bee...]
[2025-08-21 20:06:18,623|(INFO)| File: resume_cache | Message: Resume saved to cache for file hash: 7f03bf2bee...]
[2025-08-21 20:07:54,758|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:07:54,769|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:07:54,781|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:08:41,708|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:08:41,710|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:08:41,712|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:09:08,425|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:09:08,428|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:09:08,430|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:09:08,433|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:10:29,482|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:10:29,485|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:10:29,487|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:10:29,490|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:13:07,659|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:14:19,076|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:17:11,655|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:18:46,364|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/search?keywords=python&location=india]
[2025-08-21 20:18:46,369|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for file hash: 891114b445...]
[2025-08-21 20:18:46,372|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for file hash: 891114b445...]
[2025-08-21 20:18:46,375|(INFO)| File: resume_cache | Message: Cached resume retrieved for file hash: 7f03bf2bee...]
[2025-08-21 20:23:28,550|(INFO)| File: jobs_cache | Message: No cached job response found for URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 20:23:28,575|(INFO)| File: jobs_cache | Message: Job response cached for URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 20:23:28,578|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 20:23:28,581|(INFO)| File: jobs_cache | Message: No cached job response found for URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 20:23:28,586|(INFO)| File: jobs_cache | Message: Job response cached for URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 20:23:28,589|(INFO)| File: jobs_cache | Message: Cached job response retrieved for URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:17:22,005|(INFO)| File: jobs_cache | Message: No cached job response found for session:ab312a77-51f0-49df-abd9-474474108132 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:17:22,040|(INFO)| File: jobs_cache | Message: Job response cached for session:ab312a77-51f0-49df-abd9-474474108132 &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:17:22,043|(INFO)| File: jobs_cache | Message: No cached job response found for session:ab312a77-51f0-49df-abd9-474474108132 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:17:22,045|(INFO)| File: jobs_cache | Message: No cached job response found for session:ab312a77-51f0-49df-abd9-474474108132 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:17:22,048|(INFO)| File: jobs_cache | Message: Job response cached for session:ab312a77-51f0-49df-abd9-474474108132 &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:17:22,050|(INFO)| File: jobs_cache | Message: No cached job response found for session:ab312a77-51f0-49df-abd9-474474108132 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:21:57,618|(INFO)| File: jobs_cache | Message: No cached job response found for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:21:57,620|(INFO)| File: jobs_cache | Message: Job response cached for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:21:57,623|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:21:57,625|(INFO)| File: jobs_cache | Message: No cached job response found for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:21:57,627|(INFO)| File: jobs_cache | Message: Job response cached for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:21:57,629|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:fc84c3ce-fa04-46f1-99f9-a71aea9ae6e7 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:47:33,770|(INFO)| File: jobs_cache | Message: No cached job response found for session:ddd51ca5-1612-44f1-8945-9305211cb06e & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:47:33,772|(INFO)| File: jobs_cache | Message: Job response cached for session:ddd51ca5-1612-44f1-8945-9305211cb06e &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:47:33,775|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:ddd51ca5-1612-44f1-8945-9305211cb06e & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:47:33,778|(INFO)| File: jobs_cache | Message: No cached job response found for session:ddd51ca5-1612-44f1-8945-9305211cb06e & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:47:33,780|(INFO)| File: jobs_cache | Message: Job response cached for session:ddd51ca5-1612-44f1-8945-9305211cb06e &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:47:33,786|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:ddd51ca5-1612-44f1-8945-9305211cb06e & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:51:07,140|(INFO)| File: jobs_cache | Message: No cached job response found for session:7df67323-c526-41b1-8011-318da6b878f1 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:51:07,145|(INFO)| File: jobs_cache | Message: Job response cached for session:7df67323-c526-41b1-8011-318da6b878f1 &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:51:07,147|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:7df67323-c526-41b1-8011-318da6b878f1 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:51:07,149|(INFO)| File: jobs_cache | Message: No cached job response found for session:7df67323-c526-41b1-8011-318da6b878f1 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:51:07,153|(INFO)| File: jobs_cache | Message: Job response cached for session:7df67323-c526-41b1-8011-318da6b878f1 &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:51:07,158|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:7df67323-c526-41b1-8011-318da6b878f1 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:51:07,161|(INFO)| File: suggestions_cache | Message: No cached suggestions found for session:7df67323-c526-41b1-8011-318da6b878f1 & file hash: 891114b445...]
[2025-08-21 22:52:15,803|(INFO)| File: jobs_cache | Message: No cached job response found for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:52:15,806|(INFO)| File: jobs_cache | Message: Job response cached for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:52:15,808|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:52:15,810|(INFO)| File: jobs_cache | Message: No cached job response found for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:52:15,812|(INFO)| File: jobs_cache | Message: Job response cached for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:52:15,814|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:52:15,816|(INFO)| File: suggestions_cache | Message: No cached suggestions found for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 891114b445...]
[2025-08-21 22:52:15,820|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 891114b445...]
[2025-08-21 22:52:15,822|(INFO)| File: keywords_cache | Message: No cached keywords found for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 891114b445...]
[2025-08-21 22:52:15,824|(INFO)| File: keywords_cache | Message: Keywords saved to cache for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 891114b445...]
[2025-08-21 22:52:15,826|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 891114b445...]
[2025-08-21 22:52:15,829|(INFO)| File: resume_cache | Message: No cached resume found for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 7f03bf2bee...]
[2025-08-21 22:52:15,831|(INFO)| File: resume_cache | Message: Resume saved to cache for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 7f03bf2bee...]
[2025-08-21 22:52:15,834|(INFO)| File: resume_cache | Message: Cached resume retrieved for session:f1feae9b-78df-4eea-a0a5-56efeb5073ca & file hash: 7f03bf2bee...]
[2025-08-21 22:53:10,173|(INFO)| File: jobs_cache | Message: No cached job response found for session:24ae791f-c6d6-45fb-840c-427ea6375813 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:53:10,178|(INFO)| File: jobs_cache | Message: Job response cached for session:24ae791f-c6d6-45fb-840c-427ea6375813 &  URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:53:10,183|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:24ae791f-c6d6-45fb-840c-427ea6375813 & URL: https://api.jobs.com/linkedin/search?keywords=python&location=india]
[2025-08-21 22:53:10,186|(INFO)| File: jobs_cache | Message: No cached job response found for session:24ae791f-c6d6-45fb-840c-427ea6375813 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:53:10,190|(INFO)| File: jobs_cache | Message: Job response cached for session:24ae791f-c6d6-45fb-840c-427ea6375813 &  URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:53:10,194|(INFO)| File: jobs_cache | Message: Cached job response retrieved for session:24ae791f-c6d6-45fb-840c-427ea6375813 & URL: https://api.jobs.com/naukri/search?keywords=python&location=india]
[2025-08-21 22:53:10,196|(INFO)| File: suggestions_cache | Message: No cached suggestions found for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 891114b445...]
[2025-08-21 22:53:10,200|(INFO)| File: suggestions_cache | Message: Cached suggestions retrieved for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 891114b445...]
[2025-08-21 22:53:10,202|(INFO)| File: keywords_cache | Message: No cached keywords found for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 891114b445...]
[2025-08-21 22:53:10,205|(INFO)| File: keywords_cache | Message: Keywords saved to cache for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 891114b445...]
[2025-08-21 22:53:10,207|(INFO)| File: keywords_cache | Message: Cached keywords retrieved for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 891114b445...]
[2025-08-21 22:53:10,209|(INFO)| File: resume_cache | Message: No cached resume found for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 7f03bf2bee...]
[2025-08-21 22:53:10,212|(INFO)| File: resume_cache | Message: Resume saved to cache for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 7f03bf2bee...]
[2025-08-21 22:53:10,214|(INFO)| File: resume_cache | Message: Cached resume retrieved for session:24ae791f-c6d6-45fb-840c-427ea6375813 & file hash: 7f03bf2bee...]
